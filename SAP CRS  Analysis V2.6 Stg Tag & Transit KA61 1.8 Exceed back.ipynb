{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\the7490\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\the7490\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import datetime \n",
    "import qgrid\n",
    "import warnings\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data Preprocessing\n",
    "## 1.1 Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SAP VA05 File ---\n",
    "#File path\n",
    "va_file_dir = r'C:\\Users\\the7490\\MDLZ\\Central Analytics Team CAT - Projects\\Beta Quadrant\\KUNLUN\\01.Raw Data\\VA05_CSV'\n",
    "\n",
    "#get file name list\n",
    "va_file_list = os.listdir(va_file_dir)\n",
    "new_list = []\n",
    "\n",
    "\n",
    "for file in va_file_list:\n",
    "    #construct file path\n",
    "    \n",
    "    file_path = os.path.join(va_file_dir,file)\n",
    "    \n",
    "    df = pd.read_csv(file_path , encoding = 'utf-8',thousands= r',')\n",
    "    \n",
    "    df['Doc. Date'] = pd.to_datetime(df['Doc. Date'])\n",
    "\n",
    "    df = df.rename(columns=lambda x: x.strip())\n",
    "    \n",
    "    new_list.append(df)\n",
    "    # print(file)\n",
    "    \n",
    "#Combine data\n",
    "va = pd.concat(new_list)\n",
    "\n",
    "# --- CRS data ---\n",
    "# file_dir = r'C:\\Users\\the7490\\Documents\\01 RDS\\data\\crs_order'\n",
    "# file_list = os.listdir(file_dir)\n",
    "# new_list = []\n",
    "\n",
    "# for file in file_list:\n",
    "#     file_path = os.path.join(file_dir,file)\n",
    "#     dataframe = pd.read_csv(file_path)\n",
    "#     new_list.append(dataframe)\n",
    "    \n",
    "# #Combine data\n",
    "# crs = pd.concat(new_list)\n",
    "crs = pd.read_csv(r'C:\\Users\\the7490\\Downloads\\crs_updated_following.csv',encoding='utf-8')\n",
    "\n",
    "# --- Boundary data ---\n",
    "file_dir = r'C:\\Users\\the7490\\Documents\\01 RDS\\data\\min_max_intentory_date'\n",
    "\n",
    "\n",
    "file_list = os.listdir(file_dir)\n",
    "new_list = []\n",
    "\n",
    "for file in file_list:\n",
    "    file_path = os.path.join(file_dir,file)\n",
    "    dataframe = pd.read_csv(file_path)\n",
    "    new_list.append(dataframe)\n",
    "\n",
    "    \n",
    "boundary = pd.concat(new_list)\n",
    "\n",
    "# SKU mapping - Master\n",
    "sku_mapping = pd.read_excel(r'C:\\Users\\the7490\\Documents\\01 RDS\\data\\SKU Category Mapping List V2.xlsx', engine = \"xlrd\")\n",
    "sku_mapping = sku_mapping[['sku_code','Category']].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qual_quant_features(data):\n",
    "    dtypes = list(map(lambda x:str(x),list(data.dtypes)))\n",
    "    qualitative = []\n",
    "    quantitative = []\n",
    "    for i in range(len(dtypes)):\n",
    "        if dtypes[i] == 'object':\n",
    "            qualitative.append(data.columns[i])\n",
    "        else:\n",
    "            quantitative.append(data.columns[i])\n",
    "    return qualitative,quantitative\n",
    "\n",
    "qualitative,quantitative = qual_quant_features(va)\n",
    "\n",
    "# Create a function to show result\n",
    "def result_show(path):\n",
    "    return qgrid.show_grid(path,show_toolbar = True)\n",
    "\n",
    "def trim(df):\n",
    "    df_obj = df.select_dtypes(['object'])\n",
    "    df[df_obj.columns] = df_obj.apply(lambda x: x.str.strip()) #trim\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Clean Data - SAP VA05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAP处理考虑int\n",
    "# Filter out wanted columns and set data types\n",
    "va_df = va[['Rj','PO Number','Sold-To Pt','Name 1','Material','Description','Plnt','Order Qty','ConfirmQty','Doc. Date','Cust.price','Net price','SaTy']]\n",
    "# va_df = va_df[va_df['Order Qty']>0]\n",
    "\n",
    "# va_df[['Sold-To Pt','Material']] = va_df[['Sold-To Pt','Material']].astype(float).astype(int).astype(str)\n",
    "# va_df = va_df[~va_df['Material'].str.contains(r'DM')]\n",
    "# va_df[['Material']] = va_df[['Material']].astype(float)\n",
    "\n",
    "# Remove na material rows\n",
    "# va_df = va_df[~va_df['Material'].isna()]\n",
    "\n",
    "va_df[['Material','Sold-To Pt']] = va_df[['Material','Sold-To Pt']].astype(str)\n",
    "\n",
    "va_df[['Order Qty','ConfirmQty']] = va_df[['Order Qty','ConfirmQty']].fillna(0).astype(int)\n",
    "\n",
    "# Saty == ZOR  and PO number start with VZO for CRS customer\n",
    "va_df =  va_df[va_df['SaTy']=='ZOR']\n",
    "va_df = va_df[va_df['PO Number'].str.contains(r'VZO*')]\n",
    "\n",
    "# Replace null with 0 for Rj\n",
    "va_df['Rj'] = va_df['Rj'].fillna(0).astype(int)\n",
    "\n",
    "# trim\n",
    "va_df = trim(va_df)\n",
    "\n",
    "# Remove 61 code\n",
    "va_df = va_df[va_df['Rj']!= 61]\n",
    "\n",
    "# Set Rj=10 if order_qty > confrimQty and Rj is null\n",
    "# va_df['Rj'] = va_df[['Order Qty','ConfirmQty','Rj']].apply(lambda x: 10 if x['Rj']== 0 and x['Order Qty']> x['ConfirmQty'] else x['Rj'], axis =1)\n",
    "\n",
    "# Gourp by Customer, Material, Date to map CRS data and count \n",
    "va_df_grouped = va_df[['Order Qty','ConfirmQty','Sold-To Pt','Material','Doc. Date','Rj']].groupby([\"Sold-To Pt\",'Material','Doc. Date']).agg({'Order Qty':'sum','ConfirmQty':'sum','Rj':'count'}).reset_index()\n",
    "va_df_grouped.rename(columns ={'Rj':'count'}, inplace = True)\n",
    "\n",
    "\n",
    "# Get Customer and SKU Info\n",
    "va_cus = va_df[[\"Sold-To Pt\",'Name 1','Plnt']].drop_duplicates(subset=['Sold-To Pt'],keep='first',inplace=False)\n",
    "va_df_grouped_cus = pd.merge(va_df_grouped, va_cus, on = 'Sold-To Pt',how = 'left')\n",
    "\n",
    "va_sku = va_df[[\"Material\",'Description','Net price']].drop_duplicates(subset=['Material'],keep='first',inplace=False)\n",
    "df_va = pd.merge(va_df_grouped_cus, va_sku, on = 'Material',how = 'left')\n",
    "\n",
    "# df_va =  pd.merge(df_va, date[['Date','year','week']], left_on = 'Doc. Date',right_on = 'Date',how = 'left') \n",
    "df_va['year'] = df_va['Doc. Date'].dt.year.astype(int)\n",
    "df_va['week'] = df_va['Doc. Date'].apply(lambda x: x.strftime(\"%W\")).astype(int) # Start from Monday\n",
    "df_va['ds'] = df_va['Doc. Date'].apply(lambda x: x.strftime(\"%Y%m%d\")).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_va['Order Qty'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Clean data - CRS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unwanted_columns\n",
    "crs = crs.drop(columns = ['Unnamed: 0','year','week'])\n",
    "\n",
    "# Replace \\N with null\n",
    "crs = crs.replace({r'\\N': None})\n",
    "\n",
    "# Set data type\n",
    "crs['calc_date'] = pd.to_datetime(crs['calc_date'])\n",
    "crs[['suggest_box','sale_box_week1','sale_box_week2','sale_box_week3','sale_box_week4','sale_box_week5']] = crs[['suggest_box','sale_box_week1','sale_box_week2','sale_box_week3','sale_box_week4','sale_box_week5']].astype(float)\n",
    "crs[['sale_week','sale_week1','sale_week2','sale_week3','sale_week4']] = crs[['sale_week','sale_week1','sale_week2','sale_week3','sale_week4']].astype(float)\n",
    "\n",
    "crs[['sold_to_pt','material','ds']] = crs[['sold_to_pt','material','ds']].astype(str)\n",
    "crs[['order_qty','pgi_qty']] = crs[['order_qty','pgi_qty']].astype(int)\n",
    "\n",
    "crs = trim(crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Clean data - boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary.drop_duplicates(subset=['CategoryName','ReceiverCode'], keep='first',inplace=True)\n",
    "boundary = boundary.drop(columns=['CategoryCode','HotFlag','ReceiverName', 'InsertTime'])\n",
    "boundary[['ReceiverCode']] = boundary[['ReceiverCode']].astype(str)\n",
    "boundary = trim(boundary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Merge tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge crs and va\n",
    "va_crs = pd.merge(df_va, crs,left_on=['Sold-To Pt','Material','ds'],right_on=['sold_to_pt','material','ds'], how='left')\n",
    "\n",
    "# merge sku mapping to get category \n",
    "va_crs_sku = pd.merge(va_crs, sku_mapping,left_on=['material'],right_on=['sku_code'],how='left')\n",
    "\n",
    "#  merge boundary\n",
    "va_crs_sku_bdy = pd.merge(va_crs_sku, boundary,left_on=['sold_to_pt','Category'],right_on=['ReceiverCode','CategoryName'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Organize data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep = va_crs_sku_bdy\n",
    "\n",
    "# Fill NA min as 14 and max as 28\n",
    "df_prep['MinDays'] = df_prep['MinDays'].replace(np.nan, 14).astype(int)\n",
    "df_prep['MaxDays'] = df_prep['MaxDays'].replace(np.nan, 28).astype(int)\n",
    "\n",
    "# keep records of ordered date only\n",
    "df_prep = df_prep[df_prep['order_qty']>0]\n",
    "# df_prep = df_prep[df_prep['Order Qty']>0]\n",
    "\n",
    "\n",
    "# remove avg_sale < 0\n",
    "df_prep = df_prep[df_prep['avg_sales_box_day']>0]\n",
    "\n",
    "# remove duplicated columns\n",
    "df_prep = df_prep.drop(columns=['sold_to_pt','material','sku_code','ReceiverCode','calc_date'])\n",
    "\n",
    "# set data type and  replace fill na with 0\n",
    "df_prep[['suggest_box','sale_box_week1','sale_box_week2','sale_box_week3','sale_box_week4','sale_box_week5','sale_week1','sale_week2',\n",
    "         'sale_week3','sale_week4','sale_week']] = \\\n",
    "df_prep[['suggest_box','sale_box_week1','sale_box_week2','sale_box_week3','sale_box_week4','sale_box_week5','sale_week1','sale_week2','sale_week3','sale_week4','sale_week']].fillna(0).astype(float)\n",
    "\n",
    "# df_prep['sale_month'] = (df_prep['sale_box_week1']+ df_prep['sale_box_week2']+ df_prep['sale_box_week3']+ df_prep['sale_box_week4'])\n",
    "df_prep['sale_month'] = (df_prep['sale_week1']+ df_prep['sale_week2']+ df_prep['sale_week3']+ df_prep['sale_week4'])\n",
    "\n",
    "# rename columns \n",
    "df_prep = df_prep.rename(columns = {'Sold-To Pt':'sold_to_pt','Material':'material'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27073617"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prep['Order Qty'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep['MaxDays'] = df_prep['MaxDays']*1.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Tag - CRS + KA SAP CFR 94%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply shortage tag via CRS data including reason code 61 (customer real need)\n",
    "# SAP data starts from 2020\n",
    "crs['year'] = crs['calc_date'].dt.year.astype(int)\n",
    "crs['week'] = crs['calc_date'].apply(lambda x: x.strftime(\"%W\")).astype(int)   #Start from Monday\n",
    "crs_incl_61 = crs\n",
    "\n",
    "crs_incl_61 = crs_incl_61[['order_qty','pgi_qty','year','week','material']].groupby(['year','week','material']).sum().reset_index()\n",
    "\n",
    "# CFR < 94% → shorage\n",
    "crs_incl_61['Shortage_incl_61'] = crs_incl_61[['order_qty','pgi_qty']].apply(lambda x: 'shortage' if x['pgi_qty']/x['order_qty'] < 0.94 else 'normal', axis = 1)\n",
    "\n",
    "# merge\n",
    "# df_prep = pd.merge(df_prep, crs_incl_61[['year','week','material','Shortage_incl_61']], on = ['year','week','material'],how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27073617"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prep['Order Qty'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define order and pgi type\n",
    "def assign_order_type(df):\n",
    "    order = df['Order Qty']\n",
    "    inv = df['avail_inventory_box']\n",
    "    ts = df['intrans_inventory_box']\n",
    "    mx = df['MaxDays']\n",
    "    mi = df['MinDays']\n",
    "    avg = df['avg_sales_box_day']\n",
    "    \n",
    "    if order+inv +ts > mx*avg :\n",
    "        return 'over'\n",
    "    elif order+inv + ts < mi*avg :\n",
    "        return 'under'\n",
    "    else: # order+inv between(mi*avg, mx*avg)\n",
    "        return 'normal'\n",
    "    \n",
    "def assign_pgi_type(df):\n",
    "    pgi = df['ConfirmQty']\n",
    "    inv = df['avail_inventory_box']\n",
    "    ts = df['intrans_inventory_box']\n",
    "    mx = df['MaxDays']\n",
    "    mi = df['MinDays']\n",
    "    avg = df['avg_sales_box_day']\n",
    "    \n",
    "    if pgi+inv +ts > mx*avg :\n",
    "        return 'exceeded'\n",
    "    elif pgi+inv +ts < mi*avg :\n",
    "        return 'lacking'\n",
    "    else: # pgi+inv between(mi*avg, mx*avg)\n",
    "        return 'normal'\n",
    "    \n",
    "df_prep['order_type'] = df_prep.apply(assign_order_type,axis=1)\n",
    "df_prep['pgi_type'] = df_prep.apply(assign_pgi_type,axis=1)\n",
    "\n",
    "#df_prep.rename(columns = {'Shortage_incl_61': 'shortage'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crs = df_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *1.6 Add KA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ka_cus = pd.read_csv(r'C:\\Users\\the7490\\Documents\\01 RDS\\data\\dim_customer_atom.csv',encoding='gbk')\n",
    "\n",
    "\n",
    "ka = ka_cus[['sold_to_code','ka_type_bc']].astype(str)\n",
    "ka = ka[ka['ka_type_bc'].str.contains(r'NKA|LKA',regex=True)]\n",
    "ka_list =ka[['sold_to_code']].drop_duplicates().astype(int)\n",
    "\n",
    "temp_list = []\n",
    "\n",
    "for cus in zip(ka_list['sold_to_code']):\n",
    "    temp = va[va['Sold-To Pt']==cus]\n",
    "    temp_list.append(temp)\n",
    "    \n",
    "va_ka = pd.concat(temp_list)\n",
    "\n",
    "# keep records of ordered date only\n",
    "va_ka = va_ka[va_ka['Order Qty']>0]\n",
    "\n",
    "va_ka['Rj'] = va_ka['Rj'].fillna(0).astype(int)\n",
    "\n",
    "va_ka[['Sold-To Pt','Material']] = va_ka[['Sold-To Pt','Material']].astype(float).astype(int).astype(str)\n",
    "\n",
    "va_ka = trim(va_ka)\n",
    "\n",
    "# remove 61!!!\n",
    "va_ka = va_ka[va_ka['Rj']!= 61]\n",
    "\n",
    "# Remove na material rows\n",
    "va_ka = va_ka[~va_ka['Material'].isna()]\n",
    "\n",
    "va_ka[['Material','Sold-To Pt']] = va_ka[['Material','Sold-To Pt']].astype(str)\n",
    "\n",
    "# Gourp by Customer, Material, Date to map CRS data and count \n",
    "va_ka_grouped = va_ka[['Order Qty','ConfirmQty','Sold-To Pt','Material','Doc. Date','PO Number']].groupby([\"Sold-To Pt\",'Material','Doc. Date']).agg({'Order Qty':'sum','ConfirmQty':'sum','PO Number':'count'}).reset_index()\n",
    "va_ka_grouped.rename(columns ={'PO Number':'count'}, inplace = True)\n",
    "\n",
    "\n",
    "# Get Customer and SKU Info\n",
    "va_ka_cus = va_ka[[\"Sold-To Pt\",'Name 1','Plnt']].drop_duplicates(subset=['Sold-To Pt'],keep='first',inplace=False)\n",
    "va_ka_info = pd.merge(va_ka_grouped, va_ka_cus, on = 'Sold-To Pt',how = 'left')\n",
    "\n",
    "va_ka_sku = va_ka[[\"Material\",'Description','Net price']].drop_duplicates(subset=['Material'],keep='first',inplace=False)\n",
    "df_ka = pd.merge(va_ka_info, va_ka_sku, on = 'Material',how = 'left')\n",
    "\n",
    "# df_va =  pd.merge(df_va, date[['Date','year','week']], left_on = 'Doc. Date',right_on = 'Date',how = 'left') \n",
    "df_ka['year'] = df_ka['Doc. Date'].dt.year.astype(int)\n",
    "df_ka['week'] = df_ka['Doc. Date'].apply(lambda x: x.strftime(\"%W\")).astype(int)    # Start from Monday  \n",
    "df_ka['ds'] = df_ka['Doc. Date'].apply(lambda x: x.strftime(\"%Y%m%d\")).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18468161.484"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ka['Order Qty'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag update\n",
    "df_ka_tag = df_ka\n",
    "df_ka_tag.rename(columns = {'Sold-To Pt':'sold_to_pt','Material':'material'}, inplace = True)\n",
    "df_ka_tag[['sold_to_pt','material']] = df_ka_tag[['sold_to_pt','material']].astype(str)\n",
    "\n",
    "# Tag pgi type\n",
    "df_ka_tag['pgi_type'] = df_ka_tag[['Order Qty','ConfirmQty']].apply(lambda x: 'lacking' if x['Order Qty']>x['ConfirmQty'] else 'normal',axis=1)\n",
    "\n",
    "# Do not assign order type for KA as we don't sugget order qty for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## *1.7 Append \n",
    "df_crs['CusType'] = 'CRS'\n",
    "df_ka_tag['CusType']='KA'\n",
    "df = df_crs.append(df_ka_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_ka_tag = pd.concat(temp_list)\n",
    "\n",
    "# keep records of ordered date only\n",
    "va_ka_tag = va_ka_tag[va_ka_tag['Order Qty']>0]\n",
    "\n",
    "va_ka_tag['Rj'] = va_ka_tag['Rj'].fillna(0).astype(int)\n",
    "\n",
    "va_ka_tag[['Sold-To Pt','Material']] = va_ka_tag[['Sold-To Pt','Material']].astype(float).astype(int).astype(str)\n",
    "\n",
    "va_ka_tag = trim(va_ka_tag)\n",
    "\n",
    "\n",
    "# Remove na material rows\n",
    "va_ka_tag = va_ka_tag[~va_ka_tag['Material'].isna()]\n",
    "\n",
    "va_ka_tag[['Material','Sold-To Pt']] = va_ka_tag[['Material','Sold-To Pt']].astype(str)\n",
    "\n",
    "va_ka_tag.rename(columns = {'Sold-To Pt':'sold_to_pt','Material':'material'}, inplace = True)\n",
    "va_ka_tag[['sold_to_pt','material']] = va_ka_tag[['sold_to_pt','material']].astype(str)\n",
    "# # Gourp by Customer, Material, Date to map CRS data and count \n",
    "# va_ka_grouped = va_ka[['Order Qty','ConfirmQty','Sold-To Pt','Material','Doc. Date','PO Number']].groupby([\"Sold-To Pt\",'Material','Doc. Date']).agg({'Order Qty':'sum','ConfirmQty':'sum','PO Number':'count'}).reset_index()\n",
    "# va_ka_grouped.rename(columns ={'PO Number':'count'}, inplace = True)\n",
    "\n",
    "\n",
    "# # Get Customer and SKU Info\n",
    "# va_ka_cus = va_ka[[\"Sold-To Pt\",'Name 1','Plnt']].drop_duplicates(subset=['Sold-To Pt'],keep='first',inplace=False)\n",
    "# va_ka_info = pd.merge(va_ka_grouped, va_ka_cus, on = 'Sold-To Pt',how = 'left')\n",
    "\n",
    "# va_ka_sku = va_ka[[\"Material\",'Description','Net price']].drop_duplicates(subset=['Material'],keep='first',inplace=False)\n",
    "# df_ka = pd.merge(va_ka_info, va_ka_sku, on = 'Material',how = 'left')\n",
    "\n",
    "# # df_va =  pd.merge(df_va, date[['Date','year','week']], left_on = 'Doc. Date',right_on = 'Date',how = 'left') \n",
    "va_ka_tag['year'] = va_ka_tag['Doc. Date'].dt.year.astype(int)\n",
    "va_ka_tag['week'] = va_ka_tag['Doc. Date'].apply(lambda x: x.strftime(\"%W\")).astype(int)    # Start from Monday  \n",
    "# df_ka['ds'] = df_ka['Doc. Date'].apply(lambda x: x.strftime(\"%Y%m%d\")).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Shoratge tag update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_incl_61 = crs_incl_61.rename(columns = {'order_qty':'Order Qty','pgi_qty':'ConfirmQty'})\n",
    "crs_incl_61['CusType'] = 'CRS'\n",
    "df_tag = crs_incl_61.append(va_ka_tag)\n",
    "df_tag = df_tag[df_tag['Order Qty']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tag = df_tag[['Order Qty','ConfirmQty','year','week','material']].groupby(['year','week','material']).sum().reset_index()\n",
    "\n",
    "# CFR < 94% → shorage\n",
    "df_tag['shortage'] = df_tag[['Order Qty','ConfirmQty']].apply(lambda x: 'shortage' if x['ConfirmQty']/x['Order Qty'] < 0.94 else 'normal', axis = 1)\n",
    "\n",
    "# tag \n",
    "df = pd.merge(df, df_tag[['year', 'week', 'material', 'shortage']], on=['year','week','material'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge crs by 94% cfr shortage tag\n",
    "df = pd.merge(df, crs_incl_61[['year','week','material','Shortage_incl_61']], on = ['year','week','material'],how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['normal', 'shortage'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['shortage'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Allocation \n",
    "## 2.1 Calculate Gap and exceed amount for shortage records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2391939.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exceed amount: take both max boundary and following month sale into consideration \n",
    "    # exceed box = pgi - avail_inventory - max( MaxBoundary* avg_sales, sales_month)\n",
    "    # Add transitb\n",
    "exceed_pgi_box = []\n",
    "\n",
    "for ot,pt,stg,order,pgi,inv,mx,sale,avg,ts in zip(df['order_type'],df['pgi_type'],df['shortage'],df['Order Qty'],df['ConfirmQty'],df['avail_inventory_box'],df['MaxDays'],df['sale_month'],df['avg_sales_box_day'],df['intrans_inventory_box']):\n",
    "    if stg == 'shortage' and ot == 'over' and pt == 'exceeded':\n",
    "        if inv + ts > mx*avg:  # Case a:  consider max boundary\n",
    "            ex_a = pgi #exceed = pgi \n",
    "        elif inv+pgi +ts <=mx*avg:\n",
    "            ex_a = 0 # no exceed pgi\n",
    "        else: \n",
    "            ex_a = pgi + inv +ts - mx*avg\n",
    "            \n",
    "        if pgi+inv > sale: # Case b: consider sales in following month,未来一个月的情况考虑在途不公平，在途不能立马转销售\n",
    "            ex_b = pgi+inv-sale # not sell in a month\n",
    "        elif pgi+inv <= sale: #  pgi+inv <= sale sell in a month\n",
    "            ex_b = 0\n",
    "        else:\n",
    "            ex_b = None\n",
    "        exceed_pgi_box.append(int(min(ex_a,ex_b)))\n",
    "    else:\n",
    "        exceed_pgi_box.append(None)\n",
    "\n",
    "        \n",
    "df['exceed_pgi_box'] = exceed_pgi_box \n",
    "\n",
    "df['exceed_pgi_box'].sum()  #3,058,461  / 2,601,679 / transit 3,209,263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45541778.484000005"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Order Qty'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Add KA gap for crs shortage sku, gap = order - pgi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2408236.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scenario 1： \n",
    "    # normal/under ordering GAP = order - pgi - avail_inventory - transit\n",
    "    # over orderging and !exceeded pgi type GAP = max boundary * avg_sales, sale_month - pgi - avail_inventory - transit\n",
    "    # 考虑transit\n",
    "gap_box_s1 = []\n",
    "\n",
    "for ot,pt,stg,order,pgi,inv,mx,mi,avg,cus,ts in zip(df['order_type'],df['pgi_type'],df['shortage'],df['Order Qty'],df['ConfirmQty'],df['avail_inventory_box'],df['MaxDays'],df['MinDays'],df['avg_sales_box_day'],df['CusType'],df['intrans_inventory_box']):\n",
    "    if stg == 'shortage'and cus == 'CRS':\n",
    "        if ot != 'over':\n",
    "            gap_box_s1.append(round(max(order - pgi - inv -ts,0)))\n",
    "        elif ot == 'over' and pt !='exceeded': \n",
    "            gap_box_s1.append(round(max(mx*avg - pgi - inv -ts,0)))\n",
    "        else:        \n",
    "            gap_box_s1.append(None)\n",
    "    elif stg == 'shortage' and cus == 'KA':\n",
    "        gap_box_s1.append(round(order-pgi))\n",
    "    else:\n",
    "        gap_box_s1.append(None)\n",
    "        \n",
    "df['gap_box_s1'] = gap_box_s1\n",
    "df['gap_box_s1'].sum() # 443,076 -> 272162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335848.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['CusType']=='CRS'].gap_box_s1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225509.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scenario 2： \n",
    "    # normal GAP = min*avg - pgi - avail_inventory\n",
    "    # under ordering GAP = order - pgi - avail_inventory\n",
    "    # over ordering and !exceeded pgi GAP = min*avg - pgi - avail_inventory\n",
    "gap_box_s2= []\n",
    "\n",
    "for ot,pt,stg,order,pgi,inv,mx,mi,avg,cus,ts in zip(df['order_type'],df['pgi_type'],df['shortage'],df['Order Qty'],df['ConfirmQty'],df['avail_inventory_box'],df['MaxDays'],df['MinDays'],df['avg_sales_box_day'],df['CusType'],df['intrans_inventory_box']):\n",
    "    if stg == 'shortage' and cus == 'CRS':\n",
    "        if ot == 'normal':\n",
    "            gap_box_s2.append(round(max(mi*avg - pgi - inv -ts,0)))\n",
    "        elif ot == 'under': \n",
    "            gap_box_s2.append(round(max(order-pgi-inv -ts,0)))\n",
    "        elif ot == 'over' and pt !='exceeded': \n",
    "            gap_box_s2.append(round(max(mi*avg - pgi - inv -ts,0)))\n",
    "        else:\n",
    "            gap_box_s2.append(None)\n",
    "    elif stg == 'shortage' and cus == 'KA':\n",
    "        gap_box_s2.append(round(order-pgi))      \n",
    "    else:\n",
    "        gap_box_s2.append(None)\n",
    "\n",
    "df['gap_box_s2'] = gap_box_s2\n",
    "df['gap_box_s2'].sum() # 278,806 → 151,841"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153121.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['CusType']=='CRS'].gap_box_s2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2110679.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scenario 3：OWA/avail_inventory  ==0  \n",
    "    # normal GAP = min*avg - pgi - avail_inventory\n",
    "    # under ordering GAP = order - pgi - avail_inventory\n",
    "    # over ordering and normal pgi GAP = min*avg - pgi - avail_inventory\n",
    "gap_box_s3= []\n",
    "\n",
    "for ot,pt,stg,order,pgi,inv,mx,mi,avg,cus,ts in zip(df['order_type'],df['pgi_type'],df['shortage'],df['Order Qty'],df['ConfirmQty'],df['avail_inventory_box'],df['MaxDays'],df['MinDays'],df['avg_sales_box_day'],df['CusType'],df['intrans_inventory_box']):\n",
    "    if inv <= 0:\n",
    "        if stg == 'shortage' and cus == 'CRS':\n",
    "            if ot == 'normal':\n",
    "                gap_box_s3.append(round(max(mi*avg - pgi - inv -ts,0)))\n",
    "            elif ot == 'under': \n",
    "                gap_box_s3.append(round(max(order-pgi-inv -ts,0)))\n",
    "            elif ot == 'over' and pt !='exceeded': \n",
    "                gap_box_s3.append(round(max(mi*avg - pgi - inv -ts,0)))\n",
    "            else:\n",
    "                gap_box_s3.append(None)\n",
    "        else:\n",
    "            gap_box_s3.append(None)\n",
    "    elif stg == 'shortage' and cus == 'KA':\n",
    "        gap_box_s3.append(round(order-pgi))\n",
    "    else:\n",
    "        gap_box_s3.append(None)\n",
    "\n",
    "df['gap_box_s3'] = gap_box_s3\n",
    "df['gap_box_s3'].sum() # 65,147 ->  37,645"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38291.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['CusType']=='CRS'].gap_box_s3.sum()  #38291"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Align net price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_list = va_df[['Material','Net price']].groupby('Material').agg({'Net price':'mean'}).reset_index()\n",
    "sku_list_ka = va_ka[['Material','Net price']].groupby('Material').agg({'Net price':'mean'}).reset_index()\n",
    "sku_price_list = sku_list.append(sku_list_ka).drop_duplicates('Material').rename(columns = {'Material':'material'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns ='Net price')\n",
    "df = pd.merge(df,sku_price_list, on='material',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *2.2 Calculate gap can be fill by customer type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAP amount group by year, week, material and plant and customer type\n",
    "df_obj = df.select_dtypes(['object'])\n",
    "df[df_obj.columns] = df_obj.apply(lambda x: x.str.strip()) #trim\n",
    "\n",
    "sku_gap_fill = df[['year','week','material','Plnt','CusType','gap_box_s1','gap_box_s2','gap_box_s3']]\\\n",
    "                    .groupby(['year','week','material','Plnt','CusType'])\\\n",
    "                    .agg({'gap_box_s1':'sum','gap_box_s2':'sum','gap_box_s3':'sum'}).reset_index()\n",
    "sku_gap_fill['key'] = sku_gap_fill['year'].map(str).str.cat([sku_gap_fill['week'].map(str),sku_gap_fill['material'].map(str),sku_gap_fill['Plnt'].map(str)],sep='')\n",
    "\n",
    "\n",
    "# # Exceed by year,week,material,plant and to be disbribute by priority\n",
    "sku_exceed_reallocate = df[['year','week','material','Plnt','exceed_pgi_box']]\\\n",
    "                    .groupby(['year','week','material','Plnt']).agg({'exceed_pgi_box':'sum'}).reset_index()\n",
    "sku_exceed_reallocate['key'] =  sku_exceed_reallocate['year'].map(str).str.cat([sku_exceed_reallocate['week'].map(str),sku_exceed_reallocate['material'].map(str),sku_exceed_reallocate['Plnt'].map(str)],sep='')\n",
    "\n",
    "sku_exceed_gap = pd.merge(sku_gap_fill,sku_exceed_reallocate[['key','exceed_pgi_box']],on=['key'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gap can be fill by Customer Type\n",
    "\n",
    "# 1. KA\n",
    "gap_can_be_fill_s1 = []\n",
    "gap_can_be_fill_s2 = []\n",
    "gap_can_be_fill_s3 = []\n",
    "updated_exceed_pgi_s1 = []\n",
    "updated_exceed_pgi_s2 = []\n",
    "updated_exceed_pgi_s3 = []\n",
    "\n",
    "for key, cus, ex, g1, g2, g3 in zip(sku_exceed_gap['key'],sku_exceed_gap['CusType'],sku_exceed_gap['exceed_pgi_box'],\\\n",
    "                            sku_exceed_gap['gap_box_s1'],sku_exceed_gap['gap_box_s2'],sku_exceed_gap['gap_box_s3']):\n",
    "    if cus == 'KA':\n",
    "        gap_can_be_fill_s1.append(min(g1,ex))\n",
    "        gap_can_be_fill_s2.append(min(g2,ex))\n",
    "        gap_can_be_fill_s3.append(min(g3,ex))\n",
    "        \n",
    "        updated_exceed_pgi_s1.append(ex-min(g1,ex)) # update exceed_pgi\n",
    "        updated_exceed_pgi_s2.append(ex-min(g2,ex))\n",
    "        updated_exceed_pgi_s3.append(ex-min(g3,ex))\n",
    "        \n",
    "    else:\n",
    "        gap_can_be_fill_s1.append(None)\n",
    "        gap_can_be_fill_s2.append(None)\n",
    "        gap_can_be_fill_s3.append(None)\n",
    "        \n",
    "        updated_exceed_pgi_s1.append(None) #只记录更新的exceed\n",
    "        updated_exceed_pgi_s2.append(None)\n",
    "        updated_exceed_pgi_s3.append(None)\n",
    "\n",
    "sku_exceed_gap['gap_can_be_fill_s1'] = gap_can_be_fill_s1\n",
    "sku_exceed_gap['gap_can_be_fill_s2'] = gap_can_be_fill_s2\n",
    "sku_exceed_gap['gap_can_be_fill_s3'] = gap_can_be_fill_s3\n",
    "\n",
    "sku_exceed_gap['updated_exceed_pgi_s1'] = updated_exceed_pgi_s1\n",
    "sku_exceed_gap['updated_exceed_pgi_s2'] = updated_exceed_pgi_s2\n",
    "sku_exceed_gap['updated_exceed_pgi_s3'] = updated_exceed_pgi_s3\n",
    "\n",
    "\n",
    "# update exceed by KA\n",
    "sku_exceed_updated = sku_exceed_gap[sku_exceed_gap['CusType']=='KA']  \n",
    "sku_exceed_updated = sku_exceed_updated[['year','week','material','Plnt','updated_exceed_pgi_s1','updated_exceed_pgi_s2','updated_exceed_pgi_s3']]\\\n",
    "                    .groupby(['year','week','material','Plnt'])\\\n",
    "                    .agg({'updated_exceed_pgi_s1':'sum','updated_exceed_pgi_s2':'sum','updated_exceed_pgi_s3':'sum'}).reset_index()\n",
    "\n",
    "sku_exceed_updated = sku_exceed_updated.rename(columns = {'updated_exceed_pgi_s1':'ka_updated_exceed_pgi_s1',\\\n",
    "                                                          'updated_exceed_pgi_s2':'ka_updated_exceed_pgi_s2',\\\n",
    "                                                          'updated_exceed_pgi_s3':'ka_updated_exceed_pgi_s3',})\n",
    "# sku_exceed_gap = sku_exceed_gap.drop(columns = ['updated_exceed_pgi_s1','updated_exceed_pgi_s2','updated_exceed_pgi_s3'])\n",
    "sku_exceed_gap_updated = pd.merge(sku_exceed_gap,sku_exceed_updated,on=['year','week','material','Plnt'],how='left')\n",
    "sku_exceed_gap_updated['updated_exceed_s1'] = sku_exceed_gap_updated[['exceed_pgi_box','ka_updated_exceed_pgi_s1']].apply(lambda x: x['ka_updated_exceed_pgi_s1'] if x['ka_updated_exceed_pgi_s1']<x['exceed_pgi_box'] else x['exceed_pgi_box'],axis=1 )\n",
    "sku_exceed_gap_updated['updated_exceed_s2'] = sku_exceed_gap_updated[['exceed_pgi_box','ka_updated_exceed_pgi_s2']].apply(lambda x: x['ka_updated_exceed_pgi_s2'] if x['ka_updated_exceed_pgi_s2']<x['exceed_pgi_box'] else x['exceed_pgi_box'],axis=1 )\n",
    "sku_exceed_gap_updated['updated_exceed_s3'] = sku_exceed_gap_updated[['exceed_pgi_box','ka_updated_exceed_pgi_s3']].apply(lambda x: x['ka_updated_exceed_pgi_s3'] if x['ka_updated_exceed_pgi_s3']<x['exceed_pgi_box'] else x['exceed_pgi_box'],axis=1 )\n",
    "\n",
    "\n",
    "# 2. CRS\n",
    "gap_can_be_fill_s1 = []\n",
    "gap_can_be_fill_s2 = []\n",
    "gap_can_be_fill_s3 = []\n",
    "updated_exceed_pgi_s1 = []\n",
    "updated_exceed_pgi_s2 = []\n",
    "updated_exceed_pgi_s3 = []\n",
    "\n",
    "for key,cus,ex1,g1,f1,ex2,g2,f2,ex3,g3,f3 in zip(sku_exceed_gap_updated['key'],sku_exceed_gap_updated['CusType'],\\\n",
    "      sku_exceed_gap_updated['updated_exceed_s1'],sku_exceed_gap_updated['gap_box_s1'],sku_exceed_gap_updated['gap_can_be_fill_s1'],\n",
    "      sku_exceed_gap_updated['updated_exceed_s2'],sku_exceed_gap_updated['gap_box_s2'],sku_exceed_gap_updated['gap_can_be_fill_s2'],\n",
    "      sku_exceed_gap_updated['updated_exceed_s3'],sku_exceed_gap_updated['gap_box_s3'],sku_exceed_gap_updated['gap_can_be_fill_s3']):\n",
    "    \n",
    "    if cus == 'CRS':\n",
    "        gap_can_be_fill_s1.append(min(g1,ex1))\n",
    "        gap_can_be_fill_s2.append(min(g2,ex2))\n",
    "        gap_can_be_fill_s3.append(min(g3,ex3))\n",
    "\n",
    "        updated_exceed_pgi_s1.append(ex1-min(g1,ex1)) # update exceed_pgi\n",
    "        updated_exceed_pgi_s2.append(ex2-min(g2,ex2))\n",
    "        updated_exceed_pgi_s3.append(ex3-min(g3,ex3))\n",
    "    else:\n",
    "        gap_can_be_fill_s1.append(f1)\n",
    "        gap_can_be_fill_s2.append(f2)\n",
    "        gap_can_be_fill_s3.append(f3)\n",
    "\n",
    "        updated_exceed_pgi_s1.append(ex1)\n",
    "        updated_exceed_pgi_s2.append(ex2)\n",
    "        updated_exceed_pgi_s3.append(ex3)\n",
    "\n",
    "sku_exceed_gap_updated['gap_can_be_fill_s1'] = gap_can_be_fill_s1\n",
    "sku_exceed_gap_updated['gap_can_be_fill_s2'] = gap_can_be_fill_s2\n",
    "sku_exceed_gap_updated['gap_can_be_fill_s3'] = gap_can_be_fill_s3\n",
    "sku_exceed_gap_updated['updated_exceed_pgi_s1'] = updated_exceed_pgi_s1\n",
    "sku_exceed_gap_updated['updated_exceed_pgi_s2'] = updated_exceed_pgi_s2\n",
    "sku_exceed_gap_updated['updated_exceed_pgi_s3'] = updated_exceed_pgi_s3\n",
    "\n",
    "# drop assist columns\n",
    "sku_exceed_gap_updated = sku_exceed_gap_updated.drop(columns = ['updated_exceed_s1','updated_exceed_s2','updated_exceed_s3',\\\n",
    "                                                        'ka_updated_exceed_pgi_s1','ka_updated_exceed_pgi_s2','ka_updated_exceed_pgi_s3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_gap_fill = sku_exceed_gap_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gap fill index \n",
    "sku_gap_fill['gap_fill_index_s1'] = (sku_gap_fill['gap_can_be_fill_s1']/sku_gap_fill['gap_box_s1']).replace(np.inf, 0)\n",
    "sku_gap_fill['gap_fill_index_s2'] = (sku_gap_fill['gap_can_be_fill_s2']/sku_gap_fill['gap_box_s2']).replace(np.inf, 0)\n",
    "sku_gap_fill['gap_fill_index_s3'] = (sku_gap_fill['gap_can_be_fill_s3']/sku_gap_fill['gap_box_s3']).replace(np.inf, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45541778.484000005"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Order Qty'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map index back to df, by year/week/material/plant/customer type\n",
    "df1 = pd.merge(df, sku_gap_fill[['year','week','material','Plnt','CusType','gap_fill_index_s1','gap_fill_index_s2','gap_fill_index_s3']],\n",
    "               on=['year','week','material','Plnt','CusType'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gap_box_can_be_fill_s1        426706\n",
       "gap_box_can_be_fill_s2        383547\n",
       "gap_box_can_be_fill_s3        349995\n",
       "gap_can_be_fill_value_s1    64378343\n",
       "gap_can_be_fill_value_s2    57961679\n",
       "gap_can_be_fill_value_s3    53473356\n",
       "dtype: int32"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate gap can be fill value\n",
    "gap_box_can_be_fill_s1 = []\n",
    "gap_box_can_be_fill_s2 = []\n",
    "gap_box_can_be_fill_s3 = []\n",
    "gap_can_be_fill_value_s1 = []\n",
    "gap_can_be_fill_value_s2 = []\n",
    "gap_can_be_fill_value_s3 = []\n",
    "\n",
    "\n",
    "for ot,pt,stg,g1,g2,g3,ix1,ix2,ix3,price,cus in zip(df1['order_type'],df1['pgi_type'],df1['shortage'],\n",
    "                                                    df1['gap_box_s1'],df1['gap_box_s2'],df1['gap_box_s3'],\n",
    "                                df1['gap_fill_index_s1'],df1['gap_fill_index_s2'],df1['gap_fill_index_s3'],df1['Net price'],df1['CusType']):\n",
    "   \n",
    "    if stg == 'shortage' and pt != 'exceeded':\n",
    "        # calculate gap box can be fill\n",
    "        gap_box_can_be_fill_s1.append(g1*ix1)\n",
    "        gap_box_can_be_fill_s2.append(g2*ix2)\n",
    "        gap_box_can_be_fill_s3.append(g3*ix3)\n",
    "        \n",
    "        # calculate gap box value\n",
    "        gap_can_be_fill_value_s1.append(g1*ix1*price)\n",
    "        gap_can_be_fill_value_s2.append(g2*ix2*price)\n",
    "        gap_can_be_fill_value_s3.append(g3*ix3*price)\n",
    "        \n",
    "    elif stg == 'shortage' and cus == 'KA': #do allocation only for shortage sku\n",
    "        # calculate gap box can be fill\n",
    "        gap_box_can_be_fill_s1.append(g1*ix1)\n",
    "        gap_box_can_be_fill_s2.append(g2*ix2)\n",
    "        gap_box_can_be_fill_s3.append(g3*ix3)\n",
    "        \n",
    "        # calculate gap box value\n",
    "        gap_can_be_fill_value_s1.append(g1*ix1*price)\n",
    "        gap_can_be_fill_value_s2.append(g2*ix2*price)\n",
    "        gap_can_be_fill_value_s3.append(g3*ix3*price)\n",
    "        \n",
    "    else:\n",
    "        gap_box_can_be_fill_s1.append(None)\n",
    "        gap_box_can_be_fill_s2.append(None)\n",
    "        gap_box_can_be_fill_s3.append(None)\n",
    "        \n",
    "        gap_can_be_fill_value_s1.append(None)\n",
    "        gap_can_be_fill_value_s2.append(None)\n",
    "        gap_can_be_fill_value_s3.append(None)\n",
    "        \n",
    "df1['gap_box_can_be_fill_s1'] = gap_box_can_be_fill_s1\n",
    "df1['gap_box_can_be_fill_s2'] = gap_box_can_be_fill_s2\n",
    "df1['gap_box_can_be_fill_s3'] = gap_box_can_be_fill_s3\n",
    "df1['gap_can_be_fill_value_s1'] = gap_can_be_fill_value_s1\n",
    "df1['gap_can_be_fill_value_s2'] = gap_can_be_fill_value_s2\n",
    "df1['gap_can_be_fill_value_s3'] = gap_can_be_fill_value_s3\n",
    "\n",
    "# Calculate the sum\n",
    "df1[['gap_box_can_be_fill_s1','gap_box_can_be_fill_s2','gap_box_can_be_fill_s3',\n",
    "     'gap_can_be_fill_value_s1','gap_can_be_fill_value_s2','gap_can_be_fill_value_s3']].sum().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dp=df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *2.3 Calculate reallocated exceed pgi to fill the gap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAP can be fill and exceed pgi amount group by year, week, material and plant and *CusType\n",
    "        # Applied in adjusted order\n",
    "sku_exceed_reallocate = df1[['year','week','material','Plnt','exceed_pgi_box'\n",
    "                             ,'gap_box_can_be_fill_s1','gap_box_can_be_fill_s2','gap_box_can_be_fill_s3']]\\\n",
    "                                .groupby(['year','week','material','Plnt'])\\\n",
    "                                 .agg({'gap_box_can_be_fill_s1':'sum','gap_box_can_be_fill_s2':'sum','gap_box_can_be_fill_s3':'sum','exceed_pgi_box':'sum'}).reset_index()\n",
    "\n",
    "sku_exceed_reallocate['exceed_pgi_box'] = sku_exceed_reallocate['exceed_pgi_box'].astype(float)\n",
    "\n",
    "# calculate exceed index \n",
    "sku_exceed_reallocate['exceed_fill_index_s1'] = (sku_exceed_reallocate['gap_box_can_be_fill_s1']/sku_exceed_reallocate['exceed_pgi_box']).replace(np.inf,0)\n",
    "sku_exceed_reallocate['exceed_fill_index_s2'] = (sku_exceed_reallocate['gap_box_can_be_fill_s2']/sku_exceed_reallocate['exceed_pgi_box']).replace(np.inf,0)\n",
    "sku_exceed_reallocate['exceed_fill_index_s3'] = (sku_exceed_reallocate['gap_box_can_be_fill_s3']/sku_exceed_reallocate['exceed_pgi_box']).replace(np.inf,0)\n",
    "\n",
    "sku_exceed_reallocate['exceed_can_not_be_allocated_s1'] = sku_exceed_reallocate['exceed_pgi_box']-sku_exceed_reallocate['gap_box_can_be_fill_s1']\n",
    "sku_exceed_reallocate['exceed_can_not_be_allocated_s2'] = sku_exceed_reallocate['exceed_pgi_box']-sku_exceed_reallocate['gap_box_can_be_fill_s2']\n",
    "sku_exceed_reallocate['exceed_can_not_be_allocated_s3'] = sku_exceed_reallocate['exceed_pgi_box']-sku_exceed_reallocate['gap_box_can_be_fill_s3']\n",
    "\n",
    "sku_exceed_reallocate['exceed_back_index_s1'] = (sku_exceed_reallocate['exceed_can_not_be_allocated_s1']/sku_exceed_reallocate['exceed_pgi_box']).replace(np.inf,0)\n",
    "sku_exceed_reallocate['exceed_back_index_s2'] = (sku_exceed_reallocate['exceed_can_not_be_allocated_s2']/sku_exceed_reallocate['exceed_pgi_box']).replace(np.inf,0)\n",
    "sku_exceed_reallocate['exceed_back_index_s3'] = (sku_exceed_reallocate['exceed_can_not_be_allocated_s3']/sku_exceed_reallocate['exceed_pgi_box']).replace(np.inf,0)\n",
    "\n",
    "\n",
    "# Map index back to df1\n",
    "df1 = pd.merge(df1, sku_exceed_reallocate[['year','week','material','Plnt','exceed_fill_index_s1','exceed_fill_index_s2','exceed_fill_index_s3','exceed_back_index_s1','exceed_back_index_s2','exceed_back_index_s3']],\n",
    "                           on=['year','week','material','Plnt'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['exceed_fill_index_s1'] = df1[['CusType','exceed_fill_index_s1']].apply(lambda x: 0 if x['CusType']=='KA' else x['exceed_fill_index_s1'],axis=1)\n",
    "df1['exceed_fill_index_s2'] = df1[['CusType','exceed_fill_index_s2']].apply(lambda x: 0 if x['CusType']=='KA' else x['exceed_fill_index_s2'],axis=1)\n",
    "df1['exceed_fill_index_s3'] = df1[['CusType','exceed_fill_index_s3']].apply(lambda x: 0 if x['CusType']=='KA' else x['exceed_fill_index_s3'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['pgi_after_allocation_s1'] = df1['exceed_pgi_box']*df1['exceed_back_index_s1']\n",
    "df1['pgi_after_allocation_s2'] = df1['exceed_pgi_box']*df1['exceed_back_index_s2']\n",
    "df1['pgi_after_allocation_s3'] = df1['exceed_pgi_box']*df1['exceed_back_index_s3']\n",
    "\n",
    "# drop assistant columns\n",
    "df1 = df1.drop(columns = ['exceed_back_index_s1','exceed_back_index_s2','exceed_back_index_s3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Calculate adjusted order qty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42789217"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_order_qty = []\n",
    "\n",
    "for ot,pt,stg,order,inv,mx,avg,sale,cus,ts in zip(df1['order_type'],df1['pgi_type'],df1['shortage'],df1['Order Qty'],\n",
    "                                      df1['avail_inventory_box'],df1['MaxDays'],df1['avg_sales_box_day'],df1['sale_month'],df1['CusType'],df1['intrans_inventory_box']):\n",
    "    if stg == 'shortage' and ot == 'over' and cus == 'CRS':\n",
    "        if inv + ts > mx*avg: # Case 1: conside max boundary\n",
    "            aj_order_a = 0\n",
    "        else: # inv +ts <= mx*avg\n",
    "            aj_order_a = mx*avg - inv -ts\n",
    "        if order + inv  > sale: # Case: consider following month sale\n",
    "            aj_order_b = sale - inv \n",
    "        else:\n",
    "            aj_order_b = order\n",
    "        adjusted_order_qty.append(max(aj_order_a, aj_order_b))\n",
    "    else:\n",
    "        adjusted_order_qty.append(order) # no touch on KA's order\n",
    "        \n",
    "df1['adjusted_order_qty'] = adjusted_order_qty\n",
    "df1['adjusted_order_qty'].sum().astype(int) #23,539,577 → 45,431,958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43576600"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_order_qty_s1 = []\n",
    "adjusted_order_qty_s2 = []\n",
    "adjusted_order_qty_s3 = []\n",
    "\n",
    "\n",
    "for ot,pt,stg,order,cus,aj,p1,p2,p3 in zip(df1['order_type'],df1['pgi_type'],df1['shortage'],df1['Order Qty'],df1['CusType'],\n",
    "                df1['adjusted_order_qty'],df1['pgi_after_allocation_s1'],df1['pgi_after_allocation_s2'],df1['pgi_after_allocation_s3']):\n",
    "    if stg == 'shortage' and ot == 'over' and cus == 'CRS':\n",
    "        adjusted_order_qty_s1.append(max(aj, order-p1))\n",
    "        adjusted_order_qty_s2.append(max(aj, order-p2))\n",
    "        adjusted_order_qty_s3.append(max(aj, order-p3))\n",
    "\n",
    "    else:\n",
    "        adjusted_order_qty_s1.append(order) # no touch on KA's order\n",
    "        adjusted_order_qty_s2.append(order)\n",
    "        adjusted_order_qty_s3.append(order)\n",
    "        \n",
    "        \n",
    "df1['adjusted_order_qty_s1'] = adjusted_order_qty_s1\n",
    "df1['adjusted_order_qty_s2'] = adjusted_order_qty_s2\n",
    "df1['adjusted_order_qty_s3'] = adjusted_order_qty_s3\n",
    "\n",
    "df1['adjusted_order_qty_s1'].sum().astype(int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Calculate adjusted pgi qty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adjusted_confirm_qty_s1    40514606\n",
       "adjusted_confirm_qty_s2    40459773\n",
       "adjusted_confirm_qty_s3    40412349\n",
       "dtype: int32"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill na with 0 for new cols\n",
    "qualitative,quantitative = qual_quant_features(df1)\n",
    "df1[quantitative] = df1[quantitative].fillna(0)\n",
    "\n",
    "adjusted_confirm_qty_s1 = []\n",
    "adjusted_confirm_qty_s2 = []\n",
    "adjusted_confirm_qty_s3 = []\n",
    "\n",
    "for ot,pt,stg,aj1,aj2,aj3,pgi,gap1,gap2,gap3,ex, ex_ix1,ex_ix2,ex_ix3,cus \\\n",
    "    in zip(df1['order_type'],df1['pgi_type'],df1['shortage'],df1['adjusted_order_qty_s1'],df1['adjusted_order_qty_s2'],df1['adjusted_order_qty_s3'],df1['ConfirmQty'],\n",
    "           df1['gap_box_can_be_fill_s1'],df1['gap_box_can_be_fill_s2'],df1['gap_box_can_be_fill_s3'],\n",
    "           df1['exceed_pgi_box'],df1['exceed_fill_index_s1'],df1['exceed_fill_index_s2'],df1['exceed_fill_index_s3'],df1['CusType']):\n",
    "        \n",
    "        if cus == 'CRS':\n",
    "            if stg == 'normal':\n",
    "                adjusted_confirm_qty_s1.append(pgi)\n",
    "                adjusted_confirm_qty_s2.append(pgi)\n",
    "                adjusted_confirm_qty_s3.append(pgi)\n",
    "\n",
    "            elif stg == 'shortage' and pt != 'exceeded':\n",
    "                adjusted_confirm_qty_s1.append(min(pgi+gap1, aj1))\n",
    "                adjusted_confirm_qty_s2.append(min(pgi+gap2, aj2))\n",
    "                adjusted_confirm_qty_s3.append(min(pgi+gap3, aj3))\n",
    "\n",
    "            elif stg == 'shortage' and pt == 'exceeded':\n",
    "                adjusted_confirm_qty_s1.append(min(pgi-ex*ex_ix1, aj1))\n",
    "                adjusted_confirm_qty_s2.append(min(pgi-ex*ex_ix2, aj2))\n",
    "                adjusted_confirm_qty_s3.append(min(pgi-ex*ex_ix3, aj3))\n",
    "                \n",
    "        else: # cus =='KA'\n",
    "            if stg != 'shortage':\n",
    "                adjusted_confirm_qty_s1.append(pgi)\n",
    "                adjusted_confirm_qty_s2.append(pgi)\n",
    "                adjusted_confirm_qty_s3.append(pgi)\n",
    "                \n",
    "            elif stg == 'shortage':\n",
    "                adjusted_confirm_qty_s1.append(pgi+gap1)\n",
    "                adjusted_confirm_qty_s2.append(pgi+gap2)\n",
    "                adjusted_confirm_qty_s3.append(pgi+gap3)\n",
    "\n",
    "\n",
    "df1['adjusted_confirm_qty_s1'] = adjusted_confirm_qty_s1\n",
    "df1['adjusted_confirm_qty_s2'] = adjusted_confirm_qty_s2\n",
    "df1['adjusted_confirm_qty_s3'] = adjusted_confirm_qty_s3\n",
    "\n",
    "df1[['adjusted_confirm_qty_s1','adjusted_confirm_qty_s2','adjusted_confirm_qty_s3']].sum().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Calculate the benefit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 CFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shortage</th>\n",
       "      <th>CusType</th>\n",
       "      <th>original_cfr</th>\n",
       "      <th>adjusted_cfr_s1</th>\n",
       "      <th>adjusted_cfr_s2</th>\n",
       "      <th>adjusted_cfr_s3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>normal</td>\n",
       "      <td>CRS</td>\n",
       "      <td>99.708370</td>\n",
       "      <td>99.708370</td>\n",
       "      <td>99.708370</td>\n",
       "      <td>99.708370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>normal</td>\n",
       "      <td>KA</td>\n",
       "      <td>99.226985</td>\n",
       "      <td>99.226985</td>\n",
       "      <td>99.226985</td>\n",
       "      <td>99.226985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shortage</td>\n",
       "      <td>CRS</td>\n",
       "      <td>90.274161</td>\n",
       "      <td>86.125604</td>\n",
       "      <td>85.923199</td>\n",
       "      <td>85.709271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shortage</td>\n",
       "      <td>KA</td>\n",
       "      <td>80.208761</td>\n",
       "      <td>83.422698</td>\n",
       "      <td>83.422698</td>\n",
       "      <td>83.422698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shortage CusType  original_cfr  adjusted_cfr_s1  adjusted_cfr_s2  \\\n",
       "0    normal     CRS     99.708370        99.708370        99.708370   \n",
       "1    normal      KA     99.226985        99.226985        99.226985   \n",
       "2  shortage     CRS     90.274161        86.125604        85.923199   \n",
       "3  shortage      KA     80.208761        83.422698        83.422698   \n",
       "\n",
       "   adjusted_cfr_s3  \n",
       "0        99.708370  \n",
       "1        99.226985  \n",
       "2        85.709271  \n",
       "3        83.422698  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cfr = df1[['order_type','pgi_type','shortage','CusType','Order Qty','ConfirmQty','adjusted_order_qty_s1','adjusted_order_qty_s2','adjusted_order_qty_s3','adjusted_confirm_qty_s1','adjusted_confirm_qty_s2','adjusted_confirm_qty_s3']]\n",
    "\n",
    "df_cfr_groupby_shortage = df_cfr.groupby(['shortage','CusType']).sum().reset_index()\n",
    "\n",
    "df_cfr_groupby_shortage['original_cfr'] = df_cfr_groupby_shortage['ConfirmQty']/df_cfr_groupby_shortage['Order Qty']*100\n",
    "df_cfr_groupby_shortage['adjusted_cfr_s1'] = df_cfr_groupby_shortage['adjusted_confirm_qty_s1']/df_cfr_groupby_shortage['adjusted_order_qty_s1']*100\n",
    "df_cfr_groupby_shortage['adjusted_cfr_s2'] = df_cfr_groupby_shortage['adjusted_confirm_qty_s2']/df_cfr_groupby_shortage['adjusted_order_qty_s2']*100\n",
    "df_cfr_groupby_shortage['adjusted_cfr_s3'] = df_cfr_groupby_shortage['adjusted_confirm_qty_s3']/df_cfr_groupby_shortage['adjusted_order_qty_s3']*100\n",
    "\n",
    "cfr = df_cfr_groupby_shortage[['shortage','CusType','original_cfr','adjusted_cfr_s1','adjusted_cfr_s2','adjusted_cfr_s3']]\n",
    "cfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Sales \n",
    "#### CRS by allocation_value* index , KA by allocation_value * CFR% improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drive_sale_s1    54707003\n",
       "drive_sale_s2    54722451\n",
       "drive_sale_s3    53473356\n",
       "dtype: int32"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drive_sale_s1 = []\n",
    "drive_sale_s2 = []\n",
    "drive_sale_s3 = []\n",
    "\n",
    "s1_index = 0.25\n",
    "s2_index = 0.5\n",
    "s3_index = 1\n",
    "ka_index = 1 # (cfr[ (cfr['shortage']=='shortage') & (cfr['CusType']=='KA')].adjusted_cfr_s1  -cfr[ (cfr['shortage']=='shortage') & (cfr['CusType']=='KA')].original_cfr)/100\n",
    "\n",
    "for ot,pt,stg,cus, gv1,gv2,gv3 in zip(df1['order_type'],df1['pgi_type'],df1['shortage'],df1['CusType'],\n",
    "                                        df1['gap_can_be_fill_value_s1'],df1['gap_can_be_fill_value_s2'],df1['gap_can_be_fill_value_s3']):\n",
    "        \n",
    "        if stg == 'shortage' and pt != 'exceed'and cus =='CRS':\n",
    "            drive_sale_s1.append(s1_index*gv1)\n",
    "            drive_sale_s2.append(s2_index*gv2)\n",
    "            drive_sale_s3.append(s3_index*gv3)  \n",
    "            \n",
    "        elif stg == 'shortage' and cus =='KA':\n",
    "            drive_sale_s1.append(ka_index*gv1)\n",
    "            drive_sale_s2.append(ka_index*gv2)\n",
    "            drive_sale_s3.append(ka_index*gv3)\n",
    "            \n",
    "        else: \n",
    "            drive_sale_s1.append(None)\n",
    "            drive_sale_s2.append(None)\n",
    "            drive_sale_s3.append(None)\n",
    "            \n",
    "df1['drive_sale_s1']=drive_sale_s1\n",
    "df1['drive_sale_s2']=drive_sale_s2\n",
    "df1['drive_sale_s3']=drive_sale_s3\n",
    "df1[['drive_sale_s1','drive_sale_s2','drive_sale_s3']].sum().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.to_csv(r'C:\\Users\\the7490\\Downloads\\crs_processed_shortage_crs+kasap_94_transit_1.8_ka_excl_61_tag_incl61_exceed_back.csv',encoding='utf-8',index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
